{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive bayes classifier uses the bayesian theorem:\n",
    "\n",
    "$P(artist|text) = \\frac{P(text|artist)P(artist )}{P(text)}$\n",
    "\n",
    "to calculate the conditional probability: \n",
    "$P (artist | text )$ for each of the artists/classes and assigns the artist with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: for text = love and artist = Beatles\n",
    "we want to calculaye: $P(Beatles|love)$ from: \n",
    "\n",
    "- $P(love|Beatles)$ := probability of 'love' appearing in Beatles' songs\n",
    "- $P(Beatles)$ := proability of all Beatles' songs in the data set\n",
    "- $P(love)$: probability of 'love' appearing in any song in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more than 1 word, as we have many many words in our text corpus\n",
    "\n",
    "$P( text | artist ) = P( word1 | artist ) * P( word2 | artist ) * .... * P(word_N | artist)$\n",
    "\n",
    "And this where the word *naive* comes in the classifier, as it assumes all these probabilities are independent and it can just multiply them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example from yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIST_1 = 'Spice Girls'\n",
    "ARTIST_2 = 'Beatles'\n",
    "\n",
    "# here: balance your text data\n",
    "\n",
    "TEXT_CORPUS = ['last time that we had this conversation',\n",
    "               'if you wanna be my lover',\n",
    "               'spice up your life',\n",
    "               'so baby come round, come round come round',\n",
    "               'stop right now thank you very much',\n",
    "               'ill tell you what i want',\n",
    "               'what i really really want',\n",
    "               'you cant do nothing for me baby',\n",
    "               'hey you always on the run',\n",
    "               'silly games that you were playing',\n",
    "               'when I find myself in times of trouble',\n",
    "               'speaking words of wisdom',\n",
    "               'and in my hour of darkness she is standing right in front of me',\n",
    "               'whisper words of wisdom',\n",
    "               'we all live in a yellow submarine',\n",
    "               'all you need is love',\n",
    "               'eleanor rigby puts on a face that she keeps in a jar by the door',\n",
    "               'yesterday all my troubles seemed so far away',\n",
    "               'lucy in the sky with diamonds',\n",
    "               'i am the walrus']\n",
    "\n",
    "LABELS = [ARTIST_1] * 10 + [ARTIST_2] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB # for count data like the one we have with the words in texts\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def train_model(text, labels):\n",
    "    \"\"\"\n",
    "    Trains a scikit-learn classification model on text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : list\n",
    "    labels : list\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Trained scikit-learn model.\n",
    "    \n",
    "    \"\"\"\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    #here: balance the transformed numerical data\n",
    "    tf = TfidfTransformer()\n",
    "    nb = MultinomialNB(alpha = 1)\n",
    "    model = make_pipeline(cv, tf, nb)\n",
    "    model.fit(text, labels)\n",
    "    \n",
    "    return model\n",
    " \n",
    "\n",
    "def predict(model, new_text):\n",
    "    \"\"\"\n",
    "    Takes the pre-trained model pipeline and predicts new artist based on unseen text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Trained scikit-learn model pipeline.\n",
    "    new_text : str\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    prediction : str\n",
    "    \n",
    "    \"\"\"\n",
    "    new_text = [new_text]\n",
    "    prediction = model.predict(new_text)\n",
    "    probabilities = model.predict_proba(new_text)\n",
    "    \n",
    "    return prediction[0], probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(TEXT_CORPUS, LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Beatles', array([[0.5, 0.5]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, 'funny cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links from the warmup:\n",
    "- Bayesian updating: http://www.statisticalengineering.com/bayesian.htm\n",
    "- Bayes theorem with lego: https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego\n",
    "- For the visual ones among you: https://setosa.io/ev/conditional-probability/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
