{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "* Create a simulation to learn\n",
    "* Data isn't iid\n",
    "## Everything you care about can be reduced to a reward scalar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new branch of DS \n",
    "* Supervised\n",
    "* Unsupervised\n",
    "* Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical achievements in the field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Alpha Go](https://www.youtube.com/watch?v=l7ngy56GY6k)\n",
    "* [DOTA](https://www.youtube.com/watch?v=tfb6aEUMC04)\n",
    "* [Hide and Seek](https://www.youtube.com/watch?v=kopoLzvh5jY)\n",
    "* [Grid world](https://www.youtube.com/watch?v=AMnW-OsOcl8)\n",
    "* [Cartpole](https://youtu.be/XiigTGKZfks?t=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition - from Pavlov to Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Challenge - dog gets food\n",
    "* Dog hears a bell\n",
    "* Dog gets food\n",
    "* After some time, the bell contains some information about the food\n",
    "* **We should 'backfill' reward information back through time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman - dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components in a RL problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rewards - Challenge - competition - positive and negative rewards\n",
    "* Policies - Strategies\n",
    "* Environment - environment\n",
    "* Agents - Players - agents\n",
    "* States - the particular setup of the environment at some given time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SAR.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P(S_t+1 | S_t) == P(S_t+1 | S_t, S_t-1, ... S_t-n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Markov Decision Process - How is it different from a regular Markov Chain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Markov_Decision_Process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### two extra concepts: \n",
    "* action nodes - We have decision making power in this chain\n",
    "* Reward 'lines' - positive and negative rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This can be summarised as : maximize rewards!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Formally:\n",
    "\n",
    "**RL** is defined as a tuple, containing States, Actions, Transitions, Rewards, and Discounting\n",
    "\n",
    "$RL=\\{S,A,P,R,γ\\}$\n",
    "\n",
    "**S** is a set of all possible states - the State Space\n",
    "\n",
    "$S=\\{s1,s2,...,sn\\}$\n",
    "\n",
    "**A** is a set of all possible actions - the Action Space\n",
    "\n",
    "$A={a1,a2,...,am}$\n",
    "\n",
    "**P** is the probability distribution of entering state **s'** after taking action **a** in state **s**. Action is intentional, but the resulting state is sampled from a distribution\n",
    "\n",
    "$P(s′,r|s,a)$\n",
    "\n",
    "**R** is the reward recieved for taking **a** in **s**\n",
    "\n",
    "$R(a,s)$\n",
    "\n",
    "The Discount Factor is applied to future rewards. It is normally less than 1, though not guaranteed.\n",
    "\n",
    "$γ$\n",
    "\n",
    "**G** - The Total expected Reward (or **Goal**) at time **t** is the cumulated future discounted return, depending on what actions are taken.\n",
    "\n",
    "$Gt=R_{t+1}+γ∗R_{t+2}...+γ_{p−1}∗R_{t+p}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The solution space:\n",
    "#### 3 main branches:\n",
    "* Value based\n",
    "    * Predicting value\n",
    "    * Q-learning, DQN\n",
    "* Policy based\n",
    "    * Predicting policy\n",
    "    * Policy Gradient, DPN\n",
    "* Model based (Environment based)\n",
    "    * Predicting what will happen\n",
    "    * World models / MBMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sutton and Barto - RL \n",
    "* Bellman - Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do this!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an example using OpenAI's Gym\n",
    "* A handy library for learning about RL - https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install gym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work on the cartpole problem\n",
    "#### First we make an environment in which the agent can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we implement the agent-environment loop\n",
    "* Start the process by resetting the environment\n",
    "* And return an initial observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "env.reset()\n",
    "env.render()\n",
    "time.sleep(10)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same thing by taking an action - in this case a  `step` in a given direction, 0 for left and 1 for right\n",
    "* This now contains a tuple of items\n",
    "* The first is the previous observation\n",
    "* We also get a reward value\n",
    "* A boolean to tell us if we're done\n",
    "* And a value we don't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.step(1)[0]\n",
    "reward = env.step(1)[1]\n",
    "done = env.step(1)[2]\n",
    "_ = env.step(1)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0131883 ,  0.2120274 , -0.03381001, -0.34193451])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#position of the machine (- == left of screen, + == right of screen), \n",
    "# velocity of the machine, \n",
    "# angle of the pole ((- == balancing left, + == balancing right)), \n",
    "# rotation of the pole\n",
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are things we care about!!\n",
    "* Rewards  - `reward`\n",
    "* (Policies - Strategies)\n",
    "* Environment - `env`\n",
    "* Agent - `function`\n",
    "* States - `obs[2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already use the `done` boolean to work out if we can stop the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take one: Lets build an agent that takes random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent():\n",
    "    env.reset()\n",
    "    for i in range(1000):\n",
    "        env.render()\n",
    "        obs, reward, done, _ = env.step(env.action_space.sample()) # take a random action\n",
    "        time.sleep(0.1)\n",
    "        if done:\n",
    "            print(f'We survived {i} steps')\n",
    "            env.reset()\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take two: Build an agent that observes the environment and takes according action\n",
    "\n",
    "For example:\n",
    "* If the pole is left, move left\n",
    "* If the pole is right, move right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_rl():\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for i in range(1000):\n",
    "        if obs[2] < 0 :\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        time.sleep(0.1)\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            print(f'We survived {i} steps')\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We survived 39 steps\n"
     ]
    }
   ],
   "source": [
    "better_rl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take three: Use some RL techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifically, we are going to build a policy based RL algorithm\n",
    "* To build policy based RL, sample from a game simulation\n",
    "* Run multiple simulations, infer from the simulations which sets of moves results in the highest reward\n",
    "* This is on-policy - a more inefficient method than Q-learning (we need lots of samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Sample from n game simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_simulation_data(env): #training data\n",
    "    number_of_games = 200\n",
    "    last_moves = 25\n",
    "    observations = []\n",
    "    actions = []\n",
    "\n",
    "    for i in range(number_of_games):\n",
    "        game_obs = []\n",
    "        game_acts = []\n",
    "        obs = env.reset()\n",
    "\n",
    "        for j in range(1000):\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            game_obs.append(obs)\n",
    "            game_acts.append(action)\n",
    "\n",
    "            if done:\n",
    "                observations += game_obs[:-(last_moves+1)]\n",
    "                actions += game_acts[1:-last_moves]\n",
    "                break\n",
    "\n",
    "    observations = np.array(observations)\n",
    "    actions = np.array(actions)\n",
    "\n",
    "    return observations, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Train an agent to learn the policy embedded in the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sample_simulation_data(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Set the agent loose in a live simulation\n",
    "* It will act based on the best policy for a given state of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "m.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_rl(env, m):\n",
    "    #setup the game\n",
    "    obs = env.reset()\n",
    "\n",
    "    for i in range(1000):\n",
    "        #start to play the game\n",
    "        #model, tell me what to do next please\n",
    "        obs = obs.reshape(-1,4) #X data is the simulation\n",
    "        action = int(m.predict(obs)) #y data is the action we should take\n",
    "\n",
    "        #take an according step\n",
    "        obs,reward,done,_ = env.step(action)\n",
    "        #visusalise my results\n",
    "        env.render()\n",
    "        #print(obs, reward)\n",
    "        time.sleep(0.1)\n",
    "        #find out if i died\n",
    "        if done:\n",
    "            print(f'iterations survived {i}')\n",
    "            env.close()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_rl(env,m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
